# -*- coding: utf-8 -*-
"""Challenge_3_TelecomX_LATAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_r5BaT1RZWXKXkvucoNseXlLcSoS3IuM

# Extracci√≥n de datos tratados
"""

import pandas as pd

df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/Alura/Fase 3/Challenge 3/Data_limpia_TelecomX.json')

df.sample(5)

"""# **üõ†Ô∏è Preparaci√≥n de los Datos**

## Endocoding

Transforma las variables categ√≥ricas a formato num√©rico para hacerlas compatibles con los algoritmos de machine learning. Utiliza un m√©todo de codificaci√≥n adecuado, como one-hot encoding.
"""

df_encoded = df.copy()

for col in df_encoded.columns:
  if df_encoded[col].dtype == 'object' or df_encoded[col].dtype == 'bool':
    df_encoded = pd.get_dummies(df_encoded, columns=[col], prefix=col, drop_first=True)


display(df_encoded.sample(5))

"""## Verificaci√≥n de la Proporci√≥n de Cancelaci√≥n (Churn)
Calcula la proporci√≥n de clientes que cancelaron en relaci√≥n con los que permanecieron activos. Eval√∫a si existe un desbalance entre las clases, ya que esto puede impactar en los modelos predictivos y en el an√°lisis de los resultados.
"""

import seaborn as sns
import matplotlib.pyplot as plt



churn_counts = df['Churn'].value_counts()
print("Conteo de clientes por clase:")
print(churn_counts)
print("\n" + "="*40 + "\n") # Separador


churn_percentage = df['Churn'].value_counts(normalize=True) * 100
print("Proporci√≥n de clientes por clase (%):")
print(churn_percentage)
print("\n" + "="*40 + "\n") # Separador



print("Visualizaci√≥n del desbalance de clases:")
plt.figure(figsize=(8, 6))
sns.countplot(x='Churn', data=df, palette='viridis')


plt.title('Distribuci√≥n de Clientes: Activos vs. Cancelados', fontsize=16)
plt.xlabel('¬øCancel√≥ el servicio? (1 = S√≠, 0 = No)', fontsize=12)
plt.ylabel('Cantidad de Clientes', fontsize=12)
plt.xticks([0, 1], ['No (Permanecen Activos)', 'S√≠ (Cancelaron)']) # Etiquetas m√°s claras


plt.show()

"""## Balanceo de Clases (opcional)
Si deseas profundizar en el an√°lisis, aplica t√©cnicas de balanceo como undersampling o oversampling. En situaciones de fuerte desbalanceo, herramientas como SMOTE pueden ser √∫tiles para generar ejemplos sint√©ticos de la clase minoritaria.
"""

from collections import Counter
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler


df_encoded = df.copy()

for col in df_encoded.columns:
    if df_encoded[col].dtype == 'object' or df_encoded[col].dtype == 'bool':
        df_encoded = pd.get_dummies(df_encoded, columns=[col], prefix=col, drop_first=True)

X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

print(f"Distribuci√≥n de clases original: {Counter(y)}")
print("-" * 50)


print("Aplicando SMOTE (Oversampling)...")

smote = SMOTE(random_state=42)

X_smote, y_smote = smote.fit_resample(X, y)

print(f"Nueva distribuci√≥n de clases con SMOTE: {Counter(y_smote)}")
print(f"Tama√±o del dataset original: {X.shape[0]} muestras")
print(f"Tama√±o del nuevo dataset con SMOTE: {X_smote.shape[0]} muestras")
print("-" * 50)


print("Aplicando Random Under-sampling...")

rus = RandomUnderSampler(random_state=42)

X_rus, y_rus = rus.fit_resample(X, y)

print(f"Nueva distribuci√≥n de clases con Random Under-sampling: {Counter(y_rus)}")
print(f"Tama√±o del dataset original: {X.shape[0]} muestras")
print(f"Tama√±o del nuevo dataset con Under-sampling: {X_rus.shape[0]} muestras")
print("-" * 50)

"""## Normalizaci√≥n o Estandarizaci√≥n (si es necesario)
Eval√∫a la necesidad de normalizar o estandarizar los datos, seg√∫n los modelos que se aplicar√°n. Modelos basados en distancia, como KNN, SVM, Regresi√≥n Log√≠stica y Redes Neuronales, requieren este preprocesamiento. Por otro lado, modelos basados en √°rboles, como Decision Tree, Random Forest y XGBoost, no son sensibles a la escala de los datos.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(
    X_smote,
    y_smote,
    test_size=0.2,
    random_state=42,
    stratify=y_smote  # 'stratify' asegura que la proporci√≥n de clases sea igual en ambos conjuntos
)

print(f"Forma de X_train: {X_train.shape}")
print(f"Forma de X_test: {X_test.shape}")
print("-" * 50)



print("Aplicando StandardScaler...")

scaler = StandardScaler()

scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)


print("Verificaci√≥n de la estandarizaci√≥n en el conjunto de entrenamiento (primeras 5 filas):")
display(X_train_scaled.head())

print("\nEstad√≠sticas del conjunto de entrenamiento escalado (media y desviaci√≥n est√°ndar):")
display(X_train_scaled.describe().loc[['mean', 'std']].round(2))

"""# üéØ Correlaci√≥n y Selecci√≥n de Variables"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


df_encoded = df.copy()

for col in df_encoded.columns:
    if df_encoded[col].dtype == 'object' or df_encoded[col].dtype == 'bool':
        df_encoded = pd.get_dummies(df_encoded, columns=[col], prefix=col, drop_first=True)


corr_churn = df_encoded.corr()['Churn'].sort_values(ascending=False)

plt.figure(figsize=(12, 10))

sns.barplot(x=corr_churn.values, y=corr_churn.index, palette='coolwarm')

plt.title('Correlaci√≥n de las Variables con la Cancelaci√≥n (Churn)', fontsize=16)
plt.xlabel('Coeficiente de Correlaci√≥n', fontsize=12)
plt.ylabel('Variables', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.6)

plt.show()

correlation_matrix = df_encoded.corr()

plt.figure(figsize=(20, 15))


sns.heatmap(correlation_matrix, cmap='viridis', annot=False) # annot=False para no mostrar los n√∫meros

plt.title('Matriz de Correlaci√≥n Completa', fontsize=20)
plt.show()

"""## An√°lisis de Correlaci√≥n

Investiga c√≥mo variables espec√≠ficas se relacionan con la cancelaci√≥n, tales como:

Tiempo de contrato √ó Cancelaci√≥n

Gasto total √ó Cancelaci√≥n

Utiliza gr√°ficos como boxplots o scatter plots para visualizar patrones y posibles tendencias.
"""

df

import matplotlib.pyplot as plt
import seaborn as sns



print("Columnas disponibles en el DataFrame:")
print(df.columns)

df.rename(columns=lambda x: x.strip().replace(" ", ""), inplace=True)

plt.figure(figsize=(10, 6))
sns.boxplot(x="Churn", y="tenure", data=df)
plt.title("Relaci√≥n entre Tiempo de contrato y Cancelaci√≥n ")
plt.xlabel("Cancelaci√≥n ")
plt.ylabel("Tiempo de contrato (meses)")
plt.tight_layout()


plt.figure(figsize=(10, 6))
sns.boxplot(x="Churn", y="Charges.Total", data=df)
plt.title("Relaci√≥n entre Gasto Total y Cancelaci√≥n ")
plt.xlabel("Cancelaci√≥n")
plt.ylabel("Gasto Total")
plt.tight_layout()

plt.figure(figsize=(10, 6))
sns.scatterplot(x="tenure", y="Charges.Total", hue="Churn", data=df)
plt.title("Tiempo de contrato vs Gasto Total (coloreado por Churn)")
plt.xlabel("Tiempo de contrato (meses)")
plt.ylabel("Gasto Total")
plt.tight_layout()

plt.show()

"""# ü§ñ Modelado Predictivo

## Separaci√≥n de Datos

Divide el conjunto de datos en entrenamiento y prueba para evaluar el rendimiento del modelo. Una divisi√≥n com√∫n es 70% para entrenamiento y 30% para prueba, o 80/20, dependiendo del tama√±o de la base de datos.
"""

from sklearn.model_selection import train_test_split


# 80% entrenamiento, 20% prueba
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

train_df.to_csv("telecomx_train.csv", index=False)
test_df.to_csv("telecomx_test.csv", index=False)

print("Conjunto de datos dividido exitosamente en entrenamiento (80%) y prueba (20%).")

"""## Creaci√≥n de Modelos

Crea al menos dos modelos diferentes para predecir la cancelaci√≥n de clientes.

Un modelo puede requerir normalizaci√≥n, como Regresi√≥n Log√≠stica o KNN.

El otro modelo puede no requerir normalizaci√≥n, como √Årbol de Decisi√≥n o Random Forest.

üí° La decisi√≥n de aplicar o no la normalizaci√≥n depende de los modelos seleccionados. Ambos modelos pueden ser creados sin normalizaci√≥n, pero tambi√©n es una opci√≥n combinar modelos con y sin normalizaci√≥n.

Justificaci√≥n:

Regresi√≥n Log√≠stica / KNN: Estos modelos son sensibles a la escala de los datos, por lo que la normalizaci√≥n es importante para que los coeficientes o las distancias se calculen correctamente.

√Årbol de Decisi√≥n / Random Forest: Estos modelos no dependen de la escala de los datos, por lo que no es necesario aplicar normalizaci√≥n.

Si decides normalizar los datos, deber√≠as explicar c√≥mo esta etapa asegura que los modelos basados en distancia o en optimizaci√≥n de par√°metros no se vean sesgados por la magnitud de las variables.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

df = df.dropna()

df['Churn'] = df['Churn'].astype(int)

X = df.drop(columns=['Churn'])
y = df['Churn']

X = pd.get_dummies(X, drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train_scaled, y_train)

y_pred_log = log_model.predict(X_test_scaled)
print("Regresi√≥n Log√≠stica (con normalizaci√≥n):")
print("Accuracy:", accuracy_score(y_test, y_pred_log))
print(classification_report(y_test, y_pred_log))

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
print("\nRandom Forest (sin normalizaci√≥n):")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

"""La normalizaci√≥n es crucial para modelos como Regresi√≥n Log√≠stica o KNN porque estos algoritmos dependen de c√°lculos de distancia o de optimizaci√≥n de par√°metros.
Si las variables tienen escalas muy diferentes (por ejemplo, ingresos en miles y edad en decenas), las variables con mayor magnitud dominar√°n el proceso de entrenamiento.
La normalizaci√≥n (como la estandarizaci√≥n con StandardScaler) transforma todas las variables para que tengan media 0 y desviaci√≥n est√°ndar 1, asegurando que cada caracter√≠stica contribuya equitativamente al modelo.
Por otro lado, modelos como Random Forest no requieren normalizaci√≥n porque se basan en √°rboles de decisi√≥n que dividen los datos seg√∫n umbrales, sin importar la escala de las variables.

## Evaluaci√≥n de los Modelos

Eval√∫a cada modelo utilizando las siguientes m√©tricas:

Exactitud (Acur√°cia)

Precisi√≥n

Recall

F1-score

Matriz de confusi√≥n

Despu√©s, realiza un an√°lisis cr√≠tico y compara los modelos:

¬øCu√°l modelo tuvo el mejor desempe√±o?

¬øAlg√∫n modelo present√≥ overfitting o underfitting? Si es as√≠, considera las posibles causas y ajustes:

Overfitting: Cuando el modelo aprende demasiado sobre los datos de entrenamiento, perdiendo la capacidad de generalizar a nuevos datos. Considera reducir la complejidad del modelo o aumentar los datos de entrenamiento.

Underfitting: Cuando el modelo no captura bien las tendencias de los datos, lo que indica que es demasiado simple. Intenta aumentar la complejidad del modelo o ajustar sus par√°metros.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns


if 'Churn' not in df.columns:
    raise ValueError("Column 'Churn' not found in dataset.")

df_clean = df.dropna()

numeric_features = df_clean.select_dtypes(include=['int64', 'float64']).drop(columns=['Churn'])
target = df_clean['Churn']

X_train, X_test, y_train, y_test = train_test_split(numeric_features, target, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Modelo 1
log_model = LogisticRegression()
log_model.fit(X_train_scaled, y_train)
y_pred_log = log_model.predict(X_test_scaled)

# Modelo 2
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Evaluacion
def evaluate_model(y_true, y_pred, model_name):
    print(f"\nEvaluacion para {model_name}:")
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("Precision:", precision_score(y_true, y_pred))
    print("Recall:", recall_score(y_true, y_pred))
    print("F1 Score:", f1_score(y_true, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
    print("Classification Report:\n", classification_report(y_true, y_pred))


evaluate_model(y_test, y_pred_log, "Logistic Regression")
evaluate_model(y_test, y_pred_rf, "Random Forest")


fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.heatmap(confusion_matrix(y_test, y_pred_log), annot=True, fmt='d', ax=axes[0], cmap='Blues')
axes[0].set_title("Logistic Regression Confusion Matrix")
axes[0].set_xlabel("Predicted")
axes[0].set_ylabel("Actual")

sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', ax=axes[1], cmap='Greens')
axes[1].set_title("Random Forest Confusion Matrix")
axes[1].set_xlabel("Predicted")
axes[1].set_ylabel("Actual")

plt.tight_layout()
plt.savefig("confusion_matrices.png")
plt.show()

"""# üìã Interpretaci√≥n y Conclusiones

## An√°lisis de la Importancia de las Variables

Despu√©s de elegir los modelos, realiza el an√°lisis de las variables m√°s relevantes para la predicci√≥n de la cancelaci√≥n:

Regresi√≥n Log√≠stica: Investiga los coeficientes de las variables, que muestran su contribuci√≥n a la predicci√≥n de cancelaci√≥n.

KNN (K-Nearest Neighbors): Observa c√≥mo los vecinos m√°s cercanos influyen en la decisi√≥n de clasificaci√≥n. Las variables m√°s impactantes pueden ser aquellas que m√°s contribuyen a la proximidad entre los puntos de datos.

Random Forest: Utiliza la importancia de las variables proporcionada por el modelo. Random Forest calcula la importancia bas√°ndose en c√≥mo cada variable contribuye a la reducci√≥n de la impureza durante las divisiones de los √°rboles.

SVM (Support Vector Machine): En el SVM, las variables m√°s relevantes son aquellas que influyen en la frontera de decisi√≥n entre las clases. Puedes analizar los coeficientes de los vectores de soporte para entender qu√© variables tienen mayor impacto.

Otros Modelos: Dependiendo del modelo elegido, considera el an√°lisis de m√©tricas espec√≠ficas para comprender la relevancia de las variables. Por ejemplo, coeficientes en modelos lineales, pesos en redes neuronales, o la importancia relativa en t√©cnicas de boosting (como XGBoost).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC



df_clean = df.dropna()

features = df_clean.select_dtypes(include=[np.number]).drop(columns=["Churn"])
target = df_clean["Churn"]

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train_scaled, y_train)

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

svm_model = SVC(kernel='linear')
svm_model.fit(X_train_scaled, y_train)

log_coefficients = pd.Series(log_model.coef_[0], index=X_train.columns)
rf_importance = pd.Series(rf_model.feature_importances_, index=X_train.columns)
svm_coefficients = pd.Series(svm_model.coef_[0], index=X_train.columns)

plt.figure(figsize=(14, 10))

plt.subplot(2, 2, 1)
log_coefficients.sort_values().plot(kind='barh', title='Logistic Regression Coefficients')

plt.subplot(2, 2, 2)
rf_importance.sort_values().plot(kind='barh', title='Random Forest Feature Importance')

plt.subplot(2, 2, 3)
svm_coefficients.sort_values().plot(kind='barh', title='SVM Coefficients')

plt.subplot(2, 2, 4)

feature_variance = pd.Series(np.var(X_train_scaled, axis=0), index=X_train.columns)
feature_variance.sort_values().plot(kind='barh', title='KNN Feature Variance (Scaled)')

plt.tight_layout()
plt.savefig("variable_importance_models.png")
print("Se ha generado el gr√°fico 'variable_importance_models.png' con el an√°lisis de importancia de variables para los modelos.")

"""# **Conclusi√≥n**

Elaboren un informe detallado, destacando los factores que m√°s influyen en la cancelaci√≥n, bas√°ndose en las variables seleccionadas y en el rendimiento de cada modelo.

Identifiquen los principales factores que afectan la cancelaci√≥n de clientes y propongan estrategias de retenci√≥n basadas en los resultados obtenidos.

## ‚úÖ Factores m√°s influyentes seg√∫n los modelos
1. Regresi√≥n Log√≠stica
tenure (antig√ºedad del cliente) : Mayor peso negativo: clientes con menor tiempo son m√°s propensos a cancelar.
Charges.Total (cargos totales) : Influye en la probabilidad de cancelaci√≥n.
Cuentas_Diarias y Charges.Monthly (cargos mensuales) : Impactan en la decisi√≥n.
PhoneService : Relacionado con la permanencia.

2. Random Forest
Charges.Total (25.4%)
Cuentas_Diarias (21.5%)
tenure (21.2%)
Charges.Monthly (21.2%)
PaperlessBilling (facturaci√≥n electr√≥nica) : Menor, pero relevante.

3. SVM
tenure (1.13)
Cuentas_Diarias y Charges.Monthly (0.36)
PaperlessBilling (0.24)
SeniorCitizen (0.15)

4. KNN (basado en varianza)
SeniorCitizen, PhoneService, Charges.Total, Cuentas_Diarias, Partner: Variables con mayor dispersi√≥n, influyen en la proximidad entre clientes.

## üîç Conclusiones Clave


* Antig√ºedad (tenure) es el factor m√°s cr√≠tico: clientes nuevos son m√°s propensos a cancelar.

* Cargos mensuales y totales: altos costos incrementan la probabilidad de churn.

* Uso del servicio (Cuentas_Diarias): clientes con baja interacci√≥n tienden a cancelar.

* Facturaci√≥n electr√≥nica y caracter√≠sticas demogr√°ficas (SeniorCitizen, Partner) tambi√©n influyen.

## üõ†Ô∏è Estrategias de Retenci√≥n

* Programas de fidelizaci√≥n para clientes nuevos: descuentos en los primeros meses.
* Planes personalizados: reducir cargos mensuales para clientes con alto riesgo.
* Mejorar la experiencia digital: optimizar facturaci√≥n electr√≥nica y soporte online.
* Segmentaci√≥n proactiva: identificar clientes con baja interacci√≥n y ofrecer incentivos.
* Atenci√≥n especial a adultos mayores: soporte dedicado y planes adaptados.
"""

